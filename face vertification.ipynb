{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Using Siamese Network to achieve face recognition\nUsing AT & T dataset. There are in total 40 peoples, each person has 10 images. The filea are in .pgm format, h=92, w=112.\n## 1. Visulization images","metadata":{}},{"cell_type":"code","source":"import cv2\nimport os\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfiles=sorted(os.listdir('../input/att-database-of-faces/s1/'))\nimage=np.zeros((len(files),128,128,3),dtype=np.uint8)\n\nfor i,file in enumerate(files):\n    img=cv2.imread(path+file)\n    img=cv2.resize(img,(128,128))\n    image[i,:,:]=img\n\nplt.figure(figsize=(5,5))\nplt.subplot(121)\nplt.imshow(image[1,:,:])\nplt.subplot(122)\nplt.imshow(image[2,:,:])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T18:19:47.598741Z","iopub.execute_input":"2022-01-23T18:19:47.599662Z","iopub.status.idle":"2022-01-23T18:19:48.061775Z","shell.execute_reply.started":"2022-01-23T18:19:47.599613Z","shell.execute_reply":"2022-01-23T18:19:48.060799Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nclass SiameseNetwork(nn.Module):\n    def __init__(self):\n        super(SiameseNetwork, self).__init__()\n        self.cnn1 = nn.Sequential(\n                nn.ReflectionPad2d(1),\n                nn.Conv2d(1, 4, kernel_size=3),\n                nn.ReLU(inplace=True),\n                nn.BatchNorm2d(4),\n                nn.Dropout2d(p=.2),\n                  \n                nn.ReflectionPad2d(1),\n                nn.Conv2d(4, 8, kernel_size=3),\n                nn.ReLU(inplace=True),\n                nn.BatchNorm2d(8),\n                nn.Dropout2d(p=.2),\n                      \n                nn.ReflectionPad2d(1),\n                nn.Conv2d(8, 8, kernel_size=3),\n                nn.ReLU(inplace=True),\n                nn.BatchNorm2d(8),\n                nn.Dropout2d(p=.2),\n                  )\n            \n        self.fc1 = nn.Sequential(\n                nn.Linear(8*100*100, 500),\n                nn.ReLU(inplace=True),\n                  \n                nn.Linear(500, 500),\n                nn.ReLU(inplace=True),\n                  \n                nn.Linear(500, 5)\n                  )\n\n    def forward_once(self, x):\n        output = self.cnn1(x)\n        output = output.view(output.size()[0], -1)\n        output = self.fc1(output)\n        return output\n      \n    def forward(self, input1, input2):\n        output1 = self.forward_once(input1)\n        output2 = self.forward_once(input2)\n        return output1, output2\n    ","metadata":{"execution":{"iopub.status.busy":"2022-01-23T21:03:37.061240Z","iopub.execute_input":"2022-01-23T21:03:37.061635Z","iopub.status.idle":"2022-01-23T21:03:37.075638Z","shell.execute_reply.started":"2022-01-23T21:03:37.061600Z","shell.execute_reply":"2022-01-23T21:03:37.074512Z"},"trusted":true},"execution_count":73,"outputs":[]},{"cell_type":"markdown","source":"# 3. Data Processing","metadata":{}},{"cell_type":"code","source":"with open('./train.txt','w') as f:\n    data_path='../input/att-database-of-faces/'\n    for i in range(40):\n        for j in range(10):\n            img_dir=data_path+'s'+str(i+1)+'/'+str(j+1)+'.pgm'\n            f.write(img_dir+' '+str(i)+'\\n')\n    f.close()","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:37:44.501529Z","iopub.execute_input":"2022-01-23T19:37:44.502033Z","iopub.status.idle":"2022-01-23T19:37:44.509510Z","shell.execute_reply.started":"2022-01-23T19:37:44.501998Z","shell.execute_reply":"2022-01-23T19:37:44.508767Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"from torch.utils.data import Dataset,DataLoader\nfrom torchvision import transforms\nimport linecache\nfrom PIL import Image\n\ntrain_transform=transforms.Compose([\n    transforms.Resize((128,128)),\n    transforms.ToTensor()\n])\n    \nclass ATT(Dataset):\n    def __init__(self,file,transform):\n        self.transform=transform\n        self.file=file\n    def __len__(self):\n        f=open(self.file,'r')\n        return len(f.readlines())\n    def __getitem__(self,index):\n        l=linecache.getline(self.file,np.random.randint(1,self.__len__()))\n        img1_dir=l.split()\n        should_get_same_class=np.random.randint(0,1)\n        if should_get_same_class:\n            while True:\n                img2_dir=linecache.getline(self.file,np.random.randint(1,self.__len__())).split()\n                if img1_dir[1]==img2_dir[1]:\n                    break\n        else:\n            img2_dir=linecache.getline(self.file,np.random.randint(1,self.__len__())).split()\n                    \n        img1=Image.open(img1_dir[0])\n        img1=img1.convert('L')\n        img1=self.transform(img1)\n        img2=Image.open(img2_dir[0])\n        img2=img2.convert('L')\n        img2=self.transform(img2)\n        return img1,img2,torch.from_numpy(np.array([int(img1_dir[1]!=img2_dir[1])],dtype=np.float32))","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:59:34.211741Z","iopub.execute_input":"2022-01-23T19:59:34.212022Z","iopub.status.idle":"2022-01-23T19:59:34.229026Z","shell.execute_reply.started":"2022-01-23T19:59:34.211990Z","shell.execute_reply":"2022-01-23T19:59:34.228070Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"train_data=ATT('./train.txt',train_transform)\ntrain_loader=DataLoader(train_data,batch_size=32,shuffle=True)","metadata":{"execution":{"iopub.status.busy":"2022-01-23T19:38:31.981490Z","iopub.execute_input":"2022-01-23T19:38:31.981993Z","iopub.status.idle":"2022-01-23T19:38:31.987592Z","shell.execute_reply.started":"2022-01-23T19:38:31.981956Z","shell.execute_reply":"2022-01-23T19:38:31.986860Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"# 4. define contrastive loss","metadata":{}},{"cell_type":"code","source":"import torch.nn.functional as F\n# Custom Contrastive Loss\nclass ContrastiveLoss(torch.nn.Module):\n    \"\"\"\n    Contrastive loss function.\n    Based on: http://yann.lecun.com/exdb/publis/pdf/hadsell-chopra-lecun-06.pdf\n    \"\"\"\n\n    def __init__(self, margin=2.0):\n        super(ContrastiveLoss, self).__init__()\n        self.margin = margin\n\n    def forward(self, output1, output2, label):\n        euclidean_distance = F.pairwise_distance(output1, output2)\n        loss_contrastive = torch.tensor(torch.mean((1-label) * torch.pow(euclidean_distance, 2) +     # calmp夹断用法\n                                      (label)*torch.pow(torch.clamp(self.margin-euclidean_distance,min=0.0),2)),requires_grad=True)\n        return loss_contrastive","metadata":{"execution":{"iopub.status.busy":"2022-01-23T20:22:51.927107Z","iopub.execute_input":"2022-01-23T20:22:51.927614Z","iopub.status.idle":"2022-01-23T20:22:51.935465Z","shell.execute_reply.started":"2022-01-23T20:22:51.927579Z","shell.execute_reply":"2022-01-23T20:22:51.934762Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport torch.optim as optim\nfrom torch.autograd import Variable\nmodel=Siamese()\ndevice=torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\nmodel.to(device)\ncri=ContrastiveLoss()\noptimizer=optim.Adam(model.parameters(),lr=5e-4)\n\nloss_list=[]\nmin_loss=10\nfor epoch in range(50):\n    total_loss=0.0\n    model.train()\n    for i,(img1,img2,target) in enumerate(train_loader):\n        #自定义的loss,必须改成Variable或者将loss function改为torch.tensor requires_grad=True，否则无法backward\n        img1,img2,target=img1.to(device),img2.to(device),target.to(device)\n        optimizer.zero_grad()\n        output1,output2=model(img1,img2)\n        #print(output1,output2)\n        batch_loss=cri(output1,output2,target)\n        batch_loss.backward()\n        optimizer.step()\n        \n        total_loss+=batch_loss.item()\n    if total_loss<min_loss:\n        min_loss=total_loss\n        torch.save(model.state_dict(),'./loss{}.pth'.format(min_loss))\n    loss_list.append(total_loss)\n    print('epoch:',epoch,'loss:',total_loss)\n        \n","metadata":{"execution":{"iopub.status.busy":"2022-01-23T21:03:59.330998Z","iopub.execute_input":"2022-01-23T21:03:59.331554Z","iopub.status.idle":"2022-01-23T21:07:11.107184Z","shell.execute_reply.started":"2022-01-23T21:03:59.331517Z","shell.execute_reply":"2022-01-23T21:07:11.106417Z"},"trusted":true},"execution_count":74,"outputs":[]},{"cell_type":"markdown","source":"# ","metadata":{}}]}